## 8. Extensibility Hooks

### 8.1 Serialization and Custom Effects

The architecture incorporates several extensibility hooks to allow for easy modification and enhancement of the game and AI behavior. A key feature is the use of serde for serialization and deserialization of various game state types. This is particularly important for Reinforcement Learning, as it enables the export of game states, events, and transitions to formats like JSON for offline analysis, dataset generation, or model training. Serde compatibility also facilitates saving and loading game replays, which is invaluable for debugging and performance benchmarking. Another extensibility point is the PowerUpEffect trait, which can be implemented for custom power-ups. This trait would include a method like fn apply(&self, agent: &mut AgentState);. New power-ups can be registered in a global registry. The apply method would define how the power-up modifies the agent's state (e.g., increasing bomb count, blast radius, or speed). This registry-based approach means new power-ups can be introduced without modifying core game logic or AI modules, adhering to the open/closed principle.

### 8.2 Opponent Modeling and ML Integration

To support more sophisticated AI strategies, particularly those involving predictions about other agents' behavior, an OpponentModel trait is proposed: trait OpponentModel: Send { fn predict(&self, opp_id: AgentId, snap: &Snapshot) -> Vec<Pos>; }. This trait allows for the development of modules that can predict the likely future positions or actions of opponent agents. Such models could range from simple rule-based predictors (e.g., "opponent will likely move towards nearest powerup") to more complex machine learning models trained to anticipate opponent behavior based on historical data or observed patterns. The predict method would take the ID of an opponent and the current game snapshot, and return a probability distribution or a set of likely future positions. This information could then be used by the GoalManager or BombPlanner to make more informed decisions, such as setting traps or avoiding ambushes. This hook provides a clear interface for integrating various forms of opponent modeling, enhancing the strategic depth of the AI agents.

### 8.3 RL Environment and Policy Registry

The integration of Reinforcement Learning is further facilitated by specific extensibility hooks. Gym-like wrappers are provided to expose the Bomberman game as a standard RL environment. This allows the game to be easily used with a wide range of existing RL algorithms and libraries that expect the common step and reset interface. These wrappers handle the conversion between the game's internal state representation and the Observation vectors expected by RL agents, as well as managing the Reward signals and done flags. To manage the neural network models used by RL agents, a PolicyRegistry is proposed. This registry would allow NN models (e.g., TorchScript models) to be loaded dynamically at runtime based on configuration, with support for fallback to programmatic logic if a model fails to load or encounters an error. This registry can be extended to support multiple model types or versions, enabling A/B testing of different RL policies. By providing these hooks, the system can gracefully handle unexpected situations, such as a failed model load.

---

