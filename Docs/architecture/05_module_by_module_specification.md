## 5. Module-by-Module Specification

### 5.1 **state crate** *(formerly part of grid)*

The`state` crate provides the foundational data structures and logic for representing the game world. Its central component is the`GameGrid` struct, which encapsulates the entire state of the game map. This struct contains several key fields:`tiles`, representing the N×N grid of Tile objects (e.g.,`Box<[Tile; N*N]>`, potentially using const generics for fixed-size optimization);`bombs`, a collection of active bombs (e.g.,`Slab<Bomb>` for efficient ID-based lookup and storage);`agents`, a collection of AgentState objects representing all players in the game (also potentially using`Slab<AgentState>`); and a version field (e.g.,`AtomicU64`) to track the current simulation tick, crucial for synchronizing snapshots and deltas. The`GameGrid` is designed to be shared across multiple threads (e.g., wrapped in an`Arc<RwLock<GameGrid>>`) and offers a mechanism for lock-free snapshots using`crossbeam::epoch::Guard`. This ensures that AI agents can access a consistent view of the game state without incurring the overhead of mutex locking during their decision-making process.

The`GameGrid` implements a`Grid` trait, which defines common operations for querying and manipulating the grid. Key methods include`apply_delta(delta: GridDelta)` for applying incremental changes to the grid state, typically received from the game engine after each tick. To enable reactive AI agents, the`GameGrid` provides a`subscribe() -> Receiver<GridDelta>` method, allowing bot tasks to receive streams of state changes. This is essential for agents to respond to dynamic events like bomb placements, explosions, or opponent movements. A significant addition for Reinforcement Learning (RL) is the`to_observation(agent_id: AgentId) -> Vec<f32>` method. This function is responsible for serializing the relevant game state, from the perspective of a specific agent, into a flat vector of f32 values. This vector serves as the input to a neural network policy. The serialization might include one-hot encoded tile types, agent positions, bomb timers, power-up statuses, or pre-processed features like influence maps. The design of this observation space is critical for RL performance and will likely be an area of experimentation.

### 5.2 **engine crate** *(new)*

The`engine` crate encapsulates the main game loop and simulation logic. It owns the authoritative`GameGrid` instance (via`Arc<RwLock<GameGrid>>` from the`state` crate) and is responsible for advancing the simulation by one tick at 60 Hz. Key responsibilities include:
- Updating bomb timers and triggering explosions.
- Resolving agent movements and bomb placements.
- Calculating blast propagation and damage.
- Generating`GridDelta` events summarizing all state changes since the previous tick.
- Broadcasting these deltas to all subscribers (e.g., bot tasks) via a`tokio::sync::watch` channel.

The engine exposes a simple API:
```rust
pub struct Engine {
    grid: Arc<RwLock<GameGrid>>,
    delta_tx: watch::Sender<GridDelta>,
}

impl Engine {
    pub fn new(size: usize) -> (Self, watch::Receiver<GridDelta>) { ... }
    pub fn tick(&mut self) { ... } // Advances simulation by one tick
}
```

### 5.3 Snapshot Layer (integrated in state crate)

The Snapshot Layer, integrated within the state crate (e.g., in a snapshot.rs file), is responsible for providing immutable, read-only views of the GameGrid at specific points in time (versions). The primary structure is SnapshotView<'a>, which offers a zero-copy, read-only perspective of the grid’s tiles and potentially other relevant state. This immutability is crucial for the AI’s decision-making process, as it ensures that the state does not change while an agent is evaluating it, leading to more consistent and predictable behavior.

SnapshotView is backed by triomphe::Arc<[Tile]> (or a similar thread-safe, immutable reference-counted structure) for the tile data, allowing multiple bot tasks to hold references to the same snapshot without cloning the entire grid. This approach is highly efficient and ensures freedom from allocations on the hot path of AI decision-making, which is critical for meeting performance targets.

The Snapshot Layer works in conjunction with the GameGrid’s versioning system. When a bot requests a snapshot, it receives a SnapshotView corresponding to the GameGrid’s current version. This snapshot remains valid and consistent even if the GameGrid is updated by the game engine in subsequent ticks. This is typically managed using epoch-based reclamation (e.g., via crossbeam-epoch) to safely free old snapshots when no longer needed. For Reinforcement Learning, the snapshot mechanism is particularly beneficial. RL algorithms often require a consistent state for calculating Q-values or policy gradients. The SnapshotView provides exactly this. Furthermore, a new method, observe_delta(prev: &SnapshotView) -> ObservationDelta, can be introduced. This method would compute the difference between the current live GameGrid (or its latest snapshot) and a previously captured SnapshotView. The resulting ObservationDelta could then be used to create an incremental observation for an RL agent, which can be more efficient than serializing the full state every tick, especially if only small parts of the game state have changed. This incremental approach can significantly reduce the data processing overhead for the neural network.

### 5.4 Core Bot (bot crate)

The bot crate defines the Bot struct, which serves as the central coordinating unit for an individual AI agent’s decision-making and interaction with the game environment. Each Bot instance is typically associated with a unique AgentId and holds a reference to the Arc<GameGrid> (from the state crate) to access game state information. It also contains a BotConfig struct, which allows for runtime customization of the bot’s behavior, such as adjusting heuristic weights, decision thresholds (e.g., via TOML configuration files), and, crucially, a flag to enable or disable Reinforcement Learning (RL) mode (rl_mode: bool). If RL mode is active, the Bot will also hold an optional reference to an RL policy, rl_policy: Option<Arc<dyn Policy>>, which can be loaded from a specified model_path: String.

The core logic of the Bot is typically implemented in a loop (e.g., within an async task). In each iteration of this loop, corresponding to a game tick or decision cycle, the bot retrieves a current GridSnapshot from the GameGrid. Based on its configuration, it then decides whether to use programmatic AI or RL for decision making. If config.rl_mode is true, the Bot Kernel (an internal component of Bot) invokes its rl_tick(&snapshot).await method, which uses the loaded NN policy to determine an action. Otherwise, it calls kernel.tick(&snapshot).await to use the programmatic logic. Once an action (or command) is determined, the bot submits this command back to the GameGrid (or directly to the game engine via a channel) for execution. This design allows for seamless toggling between different AI paradigms without significant refactoring, facilitating comparative analysis and hybrid approaches where, for example, an RL agent might fall back to programmatic logic under certain conditions or during specific phases of learning. The Bot struct acts as the primary interface for creating, configuring, and managing the lifecycle of AI agents within the simulation.

### 5.5 Goal Manager (goals crate)

The goals crate is responsible for generating, prioritizing, and managing the high-level objectives (Goals) for an AI agent. It defines a core trait, GoalGenerator: Send + Sync, which includes a method fn generate(&self, snap: &Snapshot) -> Vec<Goal>;. This trait allows for various strategies for goal creation to be implemented and plugged into the system. Default implementations of GoalGenerator might include DestroyNearestCrate, CollectNearestPowerUp, HuntWeakestEnemy, and FleeToSafeZone. These generators analyze the current game Snapshot and propose a list of potential goals for the agent to pursue. The GoalManager itself maintains a priority queue of (Goal, Score, Plan) tuples. The Score is typically calculated by the StateEvaluator (from the scoring.rs module), and the Plan (a sequence of micro-actions) is generated by the Pathfinder and BombPlanner. Replanning is a critical aspect of the GoalManager. It is triggered under specific conditions: either when the GridDelta (representing changes in the game state) indicates a significant shift that invalidates the current plan (e.g., a new threat appears, a target is destroyed), or when the score of the current active goal drops below a certain threshold (e.g., 60% of the score of a newly identified, better goal). This adaptive replanning ensures that the agent remains responsive to the dynamic game environment. An important enhancement is the introduction of fallback mechanisms. If primary plans fail (e.g., the Pathfinder cannot find a route to the current goal), the GoalManager can revert to safer “idle” or “explore” goals to prevent the agent from becoming stuck or making suboptimal decisions. For Reinforcement Learning integration, a new RlGoalSelector implementation of GoalGenerator can be introduced. This selector would potentially use a neural network to score or directly select goals from a candidate set generated by other (potentially programmatic) goal generators, allowing learned preferences to guide high-level strategic decisions.

### 5.6 State Evaluator (integrated in goals crate)

The StateEvaluator, integrated within the goals crate (e.g., in scoring.rs), is responsible for assigning a quantitative score to potential goals or states, guiding the GoalManager in its selection process. It defines a trait StateEvaluator { fn score(&self, snap: &Snapshot, goal: &Goal) -> f32; }. This trait allows for various evaluation heuristics to be implemented and combined. Factors considered in scoring typically include the A* pathfinding distance to the goal, the safety of the path and the goal location (often by negating influence map values), the potential rewards associated with achieving the goal (e.g., value of a power-up, strategic advantage of destroying a crate), and the level of threat posed by opponents or imminent explosions. The system supports a plugin architecture, potentially using type_map::TypeMap, to allow different scoring components to be registered and combined flexibly.

A significant addition for Reinforcement Learning is the introduction of a RewardFunction trait. This trait defines RL-specific dense reward signals, which can include not only rewards for achieving sub-goals (e.g., collecting a power-up) but also shaping rewards that encourage progress towards larger objectives or survival (e.g., small positive rewards for moving towards a target, negative rewards for staying in dangerous areas). This RewardFunction would be used by the rl crate to provide feedback to the learning algorithm. Furthermore, the StateEvaluator’s capabilities can be expanded for RL by implementing an NnValueEstimator. This component would query a value network (a type of neural network) to estimate the long-term value of a given state-goal pair. This value estimate can then be used as a critical input to the scoring process, allowing the agent to leverage learned value functions for more informed decision-making, effectively integrating a critic model as seen in actor-critic RL architectures. This makes the evaluator a central point for blending heuristic knowledge with learned value estimates.

### 5.7 Pathfinder (path crate)

The path crate is dedicated to all aspects of pathfinding for the AI agents. Its core component is an A* search algorithm, typically implemented with a BinaryHeap<Node> for the open set and a Slab<Node> or similar efficient collection for the closed set. The heuristic function used by A* is crucial and often combines Manhattan distance with penalties derived from the InfluenceMap to discourage paths through dangerous areas. To handle the dynamic nature of the Bomberman environment (e.g., new bombs, destroyed crates opening up paths), the pathfinder supports incremental updates, potentially using algorithms like D* Lite. This allows previously calculated paths to be repaired more efficiently than recalculating from scratch. Beyond simple point-to-point pathfinding, the path crate also includes a macro-move planner. This component, through a function like fn expand_macro(start: Pos, macro_cmd: Macro) -> Vec<Micro>, translates high-level movement commands (e.g., “move 5 tiles north”) into a sequence of atomic micro moves. This can be optimized using lazy waypoint generation to avoid unnecessary computation.

For performance optimization, if path-cost calculations become a bottleneck, the use of SIMD (Single Instruction, Multiple Data) instructions could be explored for parallelizing certain computations within the A* algorithm, such as heuristic calculations or node comparisons. For Reinforcement Learning integration, an optional RlPathSampler can be introduced. This component would allow a neural network to guide the pathfinding process, perhaps by influencing the heuristic, suggesting preferred directions, or even sampling entire paths based on learned preferences. This could be particularly useful in complex or uncertain environments where traditional heuristics might fall short, allowing the agent to learn more nuanced navigation strategies. The RlPathSampler would provide a hook for learned behaviors to directly influence low-level movement decisions, complementing the higher-level goal selection potentially also guided by RL.

### 5.8 Bomb Planner (bombs crate)

The bombs crate encapsulates the logic related to bomb placement, explosion propagation, and safety assessment. A key feature is the caching of blast chains using a graph data structure like petgraph. This allows for efficient calculation of the total area affected by a series of chain reactions when a bomb explodes, which is crucial for both offensive and defensive planning. The crate provides functions like fn safe_tiles(pos: Pos, power: u8, snap: &Snapshot) -> Array2<bool>, which, given a potential bomb placement position, its power, and the current game snapshot, returns a 2D array (e.g., using ndarray) indicating which tiles will be safe from the resulting explosion. This is fundamental for agents to avoid self-destruction and to assess the threat posed by enemy bombs.

To determine if an agent can reach a safe tile before a bomb explodes, a Breadth-First Search (BFS) is used to check for reachable safe spots within the bomb’s timer. The performance of grid representations (ndarray versus fixed-size arrays) should be profiled, and parallelism (e.g., using rayon) can be considered for calculating blast radii or safety maps for very large or numerous simultaneous explosions. For Reinforcement Learning, a RlBombPolicy can be implemented. This policy would use a neural network to predict the probabilities of good bomb placements or to decide whether placing a bomb at the current location is advantageous. The NN could learn complex patterns related to trapping opponents, destroying specific crate configurations, or creating advantageous future board states. This allows the bomb planning, a critical tactical element in Bomberman, to be guided by learned experience rather than solely by pre-programmed heuristics, potentially leading to more sophisticated and effective bombing strategies.

### 5.9 Influence Map (influence crate)

The influence crate is responsible for generating and managing influence maps, which are 2D arrays (e.g., Array2<f32>) quantifying the danger level at each tile over a series of upcoming ticks. An InfluenceMap struct typically contains the map data itself, a dirty flag (e.g., a BitVec<N*N>) to track which tiles need updating, and a version number to synchronize with game ticks. The primary purpose of these maps is to provide agents with a spatial understanding of threats, such as active bomb blast zones and predicted explosion timings. The maps are updated incrementally; when a new bomb is placed or a change in the environment occurs, only the affected tiles (marked as dirty) and their surroundings need to be recalculated, rather than the entire map. This efficiency is crucial for performance.

A common characteristic of influence maps is the decay of influence over time and distance. For example, the influence I at a tile at time t+1 might be calculated as I(t+1) = max(0, I(t) * decay_factor – constant_decay), where decay_factor could be 0.9 and constant_decay 0.05. This models the fading danger as a bomb’s explosion recedes or its timer counts down. For scenarios with many bombs, optimizations like rayon::par_iter can be used to parallelize the update calculations across different regions of the map or for different bombs. For Reinforcement Learning, the influence map itself, or features derived from it, can be exported as part of the agent’s observation vector. Providing the NN with this pre-processed spatial danger information can significantly enhance its ability to learn safe navigation and tactical positioning, acting as a powerful form of feature engineering that simplifies the learning problem.

### 5.10 Event System (events crate)

The events crate provides a system for handling and broadcasting game events. It defines an enum, GameEvent, which enumerates various significant occurrences in the game, such as BombExplode { pos, power }, AgentDeath { id }, PowerUpCollected { id, kind }, and others. These events are crucial for multiple aspects of the AI system. Firstly, they drive the incremental updates in components like the InfluenceMap and the GoalManager’s replanning logic. Secondly, they are essential for Reinforcement Learning, as events often define rewards (e.g., positive reward for PowerUpCollected, negative for AgentDeath) and mark the end of episodes. The event system typically uses an asynchronous broadcast mechanism (e.g., tokio::sync::broadcast channel) to disseminate events to all interested subscribers, which can include logging systems, RL agents, and replay recorders.

To enhance its utility, particularly for offline RL, the event system can be expanded to include episode serialization. This means capturing streams of GridDeltas and GameEvents and saving them, for example, to JSON files. These serialized episodes can then be used for dataset generation or offline training of RL models. For RL training, the event system is further expanded to produce Transition objects. A Transition typically contains { obs, action, reward, next_obs, done } and represents a single step in an RL episode. These transitions are fed into replay buffers, which are then sampled by RL algorithms to train the neural networks. The events crate thus serves as a central nervous system for the AI, conveying critical information about state changes and game occurrences, and is a key enabler for both logging/debugging and sophisticated RL training pipelines.

### 5.11 RL Integration (rl crate)

A new rl crate is introduced to centralize all components related to Reinforcement Learning, ensuring a clean separation from the core programmatic AI logic and preventing bloat in other modules. This crate defines essential traits for RL functionality. The Policy: Send + Sync trait includes methods like fn act(&self, obs: &Vec<f32>) -> Action; for deterministic or probabilistic action selection, and fn sample(&self, obs: &Vec<f32>, epsilon: f32) -> Action; specifically for exploration, often incorporating an epsilon-greedy strategy. The ValueEstimator: Send + Sync trait provides a method fn value(&self, obs: &Vec<f32>) -> f32; for estimating the state value, crucial for algorithms like Actor-Critic or for value-based methods.

An implementation of TorchPolicy (and TorchValueEstimator) would be provided, leveraging Rust bindings for PyTorch (e.g., the tch crate). This allows loading pre-trained neural network models (e.g., CNNs for processing grid-based observations, or MLPs for feature vectors) and using them for inference directly within the Rust environment. To facilitate training, the rl crate also provides a Gym-compatible environment wrapper, BomberEnv. This struct, holding a reference to the Arc<GameGrid> (from state) and an agent_id, implements methods like fn step(&mut self, action: Action) -> (Vec<f32>, f32, bool); and fn reset(&mut self) -> Vec<f32>;. This standardized interface allows the Bomberman game to be easily integrated with existing RL libraries (e.g., Stable Baselines3, Ray RLlib) that expect a Gym-like environment. The rl crate also supports mechanisms for both external training (e.g., via gRPC if the RL logic runs in Python) and on-device training with replay buffers and training loops managed within Rust. This comprehensive approach makes the rl crate a self-contained module for all learning-related functionalities.

---

