## 2. Domain & Terminology

### 2.1 Core Game Concepts

The game is defined by a set of core concepts that form the basis of the AI agent's understanding and interaction with its environment. The Map is represented as an NÃ—N grid, where each cell is a Tile. Tiles can be empty, contain indestructible walls, destructible soft crates, or power-ups. The simulation progresses in discrete time steps called Ticks, with each tick typically lasting 16 ms, corresponding to a 60 FPS update rate. A central mechanic is the Blast Wave, which is a propagating explosion originating from a bomb. This wave expands in a Manhattan diamond shape, its extent determined by the bomb's power radius, and is halted by walls or agents. Agents can perform Macro-Moves, which are high-level sequences of actions (e.g., "advance 7 tiles east, plant bomb, withdraw west"), and Micro-Moves, which are single atomic actions such as Move::North or Move::South. The AI utilizes an Influence Map, a 2D array that quantifies the danger level at each tile over a series of upcoming ticks, to assess safety and plan movements. High-level objectives for the agent are defined as Goals, such as "demolish crate at (5, 7)" or "acquire BombUp at (10, 3)". To achieve these goals, the agent formulates a Plan, which is a chain of micro-moves and potential bomb placements. Efficient state management is achieved through State Deltas, which are compact representations of changes from the prior tick, enabling incremental computations and reducing processing overhead.

### 2.2 AI and RL Specific Terms

To bridge the gap between general game concepts and the specialized field of Reinforcement Learning (RL), the architecture employs a set of AI and RL-specific terminology. An Observation is a serialized representation of the game state, such as a flattened grid or agent statistics, formatted for input into a neural network. This is crucial for RL agents that perceive the world through these feature vectors. The agent's decision-making logic, whether programmatic or learned, is encapsulated in its Policy. A policy maps observations to action probabilities (in the case of stochastic policies) or directly to actions (for deterministic policies). For RL training, a Reward is a scalar signal derived from state changes or specific events (e.g., +1 for collecting a power-up, -10 for agent death). This reward signal guides the learning process, allowing the agent to discover strategies that maximize cumulative reward over time. The introduction of these standardized terms ensures clarity and facilitates the integration of RL libraries and frameworks, such as those following the OpenAI Gym interface, making the system more accessible for researchers and developers working in the RL domain. This common vocabulary also supports the development of hybrid agents that might combine programmatic logic with learned behaviors.

---

